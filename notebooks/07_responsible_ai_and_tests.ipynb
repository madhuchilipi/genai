{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 7: Responsible AI and Testing\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Adversarial testing\n",
    "2. Hallucination detection\n",
    "3. Robustness test harness\n",
    "4. LangSmith trace export\n",
    "5. Comprehensive evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.responsible_ai import (\n",
    "    detect_hallucination,\n",
    "    check_citation_accuracy,\n",
    "    RobustnessTestHarness,\n",
    "    create_test_report,\n",
    "    evaluate_rag_quality\n",
    ")\n",
    "from src.langsmith_integration import (\n",
    "    initialize_langsmith,\n",
    "    enable_tracing,\n",
    "    export_traces,\n",
    "    print_tracing_status\n",
    ")\n",
    "from src.rag_baseline import BaselineRAG\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Hallucination Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hallucination detection\n",
    "rag = BaselineRAG(\"../faiss_index\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "query = \"What are the key principles of GDPR?\"\n",
    "sources = rag.retrieve(query)\n",
    "answer = rag.generate_answer(query, sources)\n",
    "\n",
    "is_hallucination, score, explanation = detect_hallucination(answer, sources)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nHallucination Check:\")\n",
    "print(f\"  Overlap Score: {score:.2%}\")\n",
    "print(f\"  Is Hallucination: {is_hallucination}\")\n",
    "print(f\"  Explanation: {explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Citation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check citation accuracy\n",
    "citation_result = check_citation_accuracy(answer, sources)\n",
    "\n",
    "print(f\"Citation Analysis:\")\n",
    "print(f\"  Total citations: {citation_result['num_citations']}\")\n",
    "print(f\"  Valid citations: {citation_result['valid_citations']}\")\n",
    "print(f\"  Accuracy: {citation_result['accuracy']:.2%}\")\n",
    "print(f\"  Has citations: {citation_result['has_citations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Adversarial Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run adversarial tests\n",
    "harness = RobustnessTestHarness(rag)\n",
    "adversarial_results = harness.run_adversarial_tests()\n",
    "\n",
    "# Summary\n",
    "safe_count = sum(1 for r in adversarial_results if r.get('is_safe', False))\n",
    "print(f\"\\nAdversarial Test Summary:\")\n",
    "print(f\"  Total tests: {len(adversarial_results)}\")\n",
    "print(f\"  Passed safely: {safe_count}\")\n",
    "print(f\"  Pass rate: {safe_count/len(adversarial_results):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Edge Case Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run edge case tests\n",
    "edge_results = harness.run_edge_case_tests()\n",
    "\n",
    "# Summary\n",
    "graceful_count = sum(1 for r in edge_results if r.get('handles_gracefully', False))\n",
    "print(f\"\\nEdge Case Test Summary:\")\n",
    "print(f\"  Total tests: {len(edge_results)}\")\n",
    "print(f\"  Handled gracefully: {graceful_count}\")\n",
    "print(f\"  Success rate: {graceful_count/len(edge_results):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Consistency Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test answer consistency\n",
    "consistency_result = harness.run_consistency_tests(\n",
    "    \"What are the key principles of GDPR?\",\n",
    "    num_runs=3\n",
    ")\n",
    "\n",
    "print(f\"\\nConsistency Test:\")\n",
    "print(f\"  Query: {consistency_result['query']}\")\n",
    "print(f\"  Runs: {consistency_result['num_runs']}\")\n",
    "print(f\"  Unique answers: {consistency_result['unique_answers']}\")\n",
    "print(f\"  Consistency score: {consistency_result['consistency_score']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on multiple queries\n",
    "test_queries = [\n",
    "    \"What are the key principles of GDPR?\",\n",
    "    \"What rights do individuals have?\",\n",
    "    \"What are the penalties for violations?\"\n",
    "]\n",
    "\n",
    "evaluation = evaluate_rag_quality(rag, test_queries)\n",
    "\n",
    "print(f\"\\nEvaluation Summary:\")\n",
    "print(f\"  Total queries: {evaluation['total_queries']}\")\n",
    "print(f\"  Successful: {evaluation['successful_queries']}\")\n",
    "print(f\"  Avg overlap score: {evaluation['avg_overlap_score']:.2%}\")\n",
    "print(f\"  Hallucination rate: {evaluation['hallucination_rate']:.2%}\")\n",
    "print(f\"  Avg citation accuracy: {evaluation['avg_citation_accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Complete Test Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive test report\n",
    "report = create_test_report(rag, output_path=\"../test_report.json\")\n",
    "\n",
    "print(f\"\\nTest report generated with:\")\n",
    "print(f\"  Adversarial tests: {len(report['adversarial_tests'])}\")\n",
    "print(f\"  Edge case tests: {len(report['edge_case_tests'])}\")\n",
    "print(f\"  Total tests: {report['summary']['total_tests']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: LangSmith Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check LangSmith status\n",
    "print_tracing_status()\n",
    "\n",
    "# Initialize LangSmith (optional)\n",
    "client = initialize_langsmith(project_name=\"gdpr-rag-evaluation\")\n",
    "\n",
    "if client:\n",
    "    print(\"LangSmith client initialized\")\n",
    "else:\n",
    "    print(\"LangSmith not available (set LANGSMITH_API_KEY to enable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export traces (if available)\n",
    "if client:\n",
    "    success = export_traces(\n",
    "        client,\n",
    "        \"gdpr-rag-evaluation\",\n",
    "        \"../traces_export.json\",\n",
    "        limit=50\n",
    "    )\n",
    "    print(f\"Trace export: {'✓ Success' if success else '✗ Failed'}\")\n",
    "else:\n",
    "    print(\"Trace export skipped (no LangSmith client)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✓ Tested hallucination detection\n",
    "✓ Verified citation accuracy\n",
    "✓ Ran adversarial tests\n",
    "✓ Tested edge cases\n",
    "✓ Evaluated consistency\n",
    "✓ Generated comprehensive report\n",
    "✓ Integrated LangSmith tracing\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review test report at `../test_report.json`\n",
    "2. Set LANGSMITH_API_KEY for production tracing\n",
    "3. Add more test cases as needed\n",
    "4. Deploy with appropriate guardrails\n",
    "5. Monitor production performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
