{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Milestone 2: Baseline RAG Pipeline\n",
        "\n",
        "This notebook implements a baseline RAG (Retrieval-Augmented Generation) pipeline:\n",
        "1. Load FAISS index from disk\n",
        "2. Implement query → retrieval workflow\n",
        "3. Format prompts with retrieved context\n",
        "4. Generate answers using LLM\n",
        "5. Evaluate baseline performance\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from src import rag_baseline\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"RAG baseline module loaded successfully!\")\n",
        "print(f\"OpenAI API Key present: {bool(os.getenv('OPENAI_API_KEY'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Initialize Baseline RAG\n",
        "\n",
        "Create an instance of the BaselineRAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize RAG system\n",
        "rag = rag_baseline.BaselineRAG(\n",
        "    faiss_path=\"../faiss_index\",\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    top_k=3\n",
        ")\n",
        "\n",
        "print(\"✅ BaselineRAG initialized\")\n",
        "print(f\"Model: {rag.model}\")\n",
        "print(f\"Top K: {rag.top_k}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Test Retrieval\n",
        "\n",
        "Test the retrieval component independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test retrieval with sample query\n",
        "query = \"What is personal data according to GDPR?\"\n",
        "\n",
        "retrieved_docs = rag.retrieve(query)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"\\nRetrieved {len(retrieved_docs)} documents:\")\n",
        "\n",
        "for i, doc in enumerate(retrieved_docs, 1):\n",
        "    print(f\"\\n{i}. Article {doc['metadata'].get('article', 'N/A')}\")\n",
        "    print(f\"   Content: {doc['content'][:100]}...\")\n",
        "    print(f\"   Score: {doc.get('score', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Test Prompt Formatting\n",
        "\n",
        "See how the prompt is formatted with retrieved context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format prompt\n",
        "prompt = rag.format_prompt(query, retrieved_docs)\n",
        "\n",
        "print(\"Formatted prompt:\")\n",
        "print(\"=\" * 60)\n",
        "print(prompt)\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run Complete RAG Pipeline\n",
        "\n",
        "Execute the full query pipeline: retrieve → format → generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run complete pipeline\n",
        "result = rag.query(query)\n",
        "\n",
        "print(\"RAG Pipeline Result:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Question: {result['question']}\")\n",
        "print(f\"\\nAnswer: {result['answer']}\")\n",
        "print(f\"\\nSources: {result['num_sources']} documents\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Test with Multiple Queries\n",
        "\n",
        "Evaluate the system with different types of questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test queries\n",
        "test_queries = [\n",
        "    \"What is personal data according to GDPR?\",\n",
        "    \"What are the main principles of GDPR?\",\n",
        "    \"What rights do data subjects have?\",\n",
        "    \"What is the territorial scope of GDPR?\"\n",
        "]\n",
        "\n",
        "print(\"Testing multiple queries:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, q in enumerate(test_queries, 1):\n",
        "    print(f\"\\n{i}. {q}\")\n",
        "    result = rag.query(q)\n",
        "    print(f\"   Answer: {result['answer'][:150]}...\")\n",
        "    print(f\"   Sources: {result['num_sources']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Evaluation Metrics\n",
        "\n",
        "Calculate basic performance metrics for the baseline system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple evaluation metrics\n",
        "import time\n",
        "\n",
        "eval_queries = test_queries[:2]  # Use subset for quick evaluation\n",
        "latencies = []\n",
        "\n",
        "print(\"Evaluating baseline performance:\")\n",
        "\n",
        "for query in eval_queries:\n",
        "    start_time = time.time()\n",
        "    result = rag.query(query)\n",
        "    latency = time.time() - start_time\n",
        "    latencies.append(latency)\n",
        "    \n",
        "    print(f\"\\nQuery: {query[:40]}...\")\n",
        "    print(f\"Latency: {latency:.2f}s\")\n",
        "    print(f\"Answer length: {len(result['answer'])} chars\")\n",
        "\n",
        "print(f\"\\nAverage latency: {sum(latencies)/len(latencies):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we:\n",
        "- ✅ Initialized the baseline RAG system\n",
        "- ✅ Tested retrieval functionality\n",
        "- ✅ Examined prompt formatting\n",
        "- ✅ Ran complete RAG pipeline\n",
        "- ✅ Evaluated with multiple queries\n",
        "- ✅ Measured baseline performance metrics\n",
        "\n",
        "Next: Proceed to `03_memory_integration.ipynb` to add conversational memory."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
