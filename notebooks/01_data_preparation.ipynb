{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Preparation\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Downloading the GDPR PDF\n",
    "2. Parsing with LangChain loaders\n",
    "3. Implementing different chunking strategies\n",
    "4. Generating embeddings with OpenAI\n",
    "5. Building and persisting FAISS index\n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure to set your `OPENAI_API_KEY` environment variable:\n",
    "```bash\n",
    "export OPENAI_API_KEY='your-key-here'\n",
    "```\n",
    "\n",
    "Or create a `.env` file with:\n",
    "```\n",
    "OPENAI_API_KEY=your-key-here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data_prep import (\n",
    "    download_gdpr_pdf,\n",
    "    load_and_split,\n",
    "    build_and_persist_faiss,\n",
    "    get_chunk_statistics\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"OpenAI API Key set: {bool(os.getenv('OPENAI_API_KEY'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download GDPR PDF\n",
    "\n",
    "Download the GDPR regulation PDF from the EU official source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GDPR PDF\n",
    "pdf_path = download_gdpr_pdf(\"../data/gdpr.pdf\")\n",
    "print(f\"PDF saved to: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Split Document\n",
    "\n",
    "Test different chunking strategies:\n",
    "- Paragraph-based\n",
    "- Article-based\n",
    "- Token-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try paragraph-based chunking\n",
    "print(\"\\n=== Paragraph-based Chunking ===\")\n",
    "docs_paragraph = load_and_split(pdf_path, strategy=\"paragraph\")\n",
    "stats_paragraph = get_chunk_statistics(docs_paragraph)\n",
    "print(f\"Chunks: {stats_paragraph['count']}\")\n",
    "print(f\"Avg length: {stats_paragraph['avg_length']:.0f} chars\")\n",
    "print(f\"\\nFirst chunk preview:\\n{docs_paragraph[0]['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try token-based chunking\n",
    "print(\"\\n=== Token-based Chunking ===\")\n",
    "docs_token = load_and_split(pdf_path, strategy=\"token\", chunk_size=500, chunk_overlap=100)\n",
    "stats_token = get_chunk_statistics(docs_token)\n",
    "print(f\"Chunks: {stats_token['count']}\")\n",
    "print(f\"Avg length: {stats_token['avg_length']:.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Chunk Statistics\n",
    "\n",
    "Compare different chunking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare strategies\n",
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\"strategy\": \"paragraph\", **stats_paragraph},\n",
    "    {\"strategy\": \"token\", **stats_token}\n",
    "])\n",
    "\n",
    "print(\"\\n=== Chunking Strategy Comparison ===\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build FAISS Index\n",
    "\n",
    "Generate embeddings and build FAISS vector store.\n",
    "\n",
    "**Note:** This requires a valid OpenAI API key. Without it, a mock index will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS index with paragraph chunks\n",
    "faiss_path = build_and_persist_faiss(\n",
    "    docs_paragraph,\n",
    "    \"../faiss_index\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(f\"\\nFAISS index saved to: {faiss_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "- ✓ Downloaded the GDPR PDF\n",
    "- ✓ Tested different chunking strategies\n",
    "- ✓ Analyzed chunk statistics\n",
    "- ✓ Built and persisted FAISS index\n",
    "\n",
    "Next: Move to `02_rag_baseline.ipynb` to implement the baseline RAG pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
