{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Milestone 1: Data Preparation\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Downloading the GDPR PDF from official sources\n",
        "2. Parsing with LangChain loaders\n",
        "3. Implementing chunking strategies (paragraph, article, token-based)\n",
        "4. Generating embeddings using OpenAI\n",
        "5. Building and persisting FAISS vector store\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Set your OpenAI API key:\n",
        "```bash\n",
        "export OPENAI_API_KEY='your-key-here'\n",
        "```\n",
        "\n",
        "Or create a `.env` file:\n",
        "```\n",
        "OPENAI_API_KEY=your-key-here\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "from src.data_prep import download_gdpr_pdf, load_and_split, build_and_persist_faiss, get_embedding_stats\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"\u2713 Modules imported successfully\")\n",
        "print(f\"\u2713 OpenAI API key set: {bool(os.environ.get('OPENAI_API_KEY'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Download GDPR PDF\n",
        "\n",
        "Download the official GDPR regulation from the EU website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download GDPR PDF\n",
        "pdf_path = download_gdpr_pdf(\"gdpr.pdf\")\n",
        "print(f\"PDF path: {pdf_path}\")\n",
        "\n",
        "# Note: In dry-run mode (no API key), this returns a placeholder path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and Split Document\n",
        "\n",
        "Parse the PDF and split into chunks using different strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy 1: Paragraph-based splitting\n",
        "docs_paragraph = load_and_split(pdf_path, strategy=\"paragraph\", chunk_size=1000)\n",
        "print(f\"Paragraph strategy: {len(docs_paragraph)} chunks\")\n",
        "\n",
        "# Strategy 2: Token-based splitting\n",
        "docs_token = load_and_split(pdf_path, strategy=\"token\", chunk_size=512, chunk_overlap=50)\n",
        "print(f\"Token strategy: {len(docs_token)} chunks\")\n",
        "\n",
        "# Strategy 3: Article-based splitting (GDPR-specific)\n",
        "docs_article = load_and_split(pdf_path, strategy=\"article\")\n",
        "print(f\"Article strategy: {len(docs_article)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Analyze Chunks\n",
        "\n",
        "Get statistics about the document chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get statistics\n",
        "stats = get_embedding_stats(docs_paragraph)\n",
        "print(f\"Statistics for paragraph-based chunks:\")\n",
        "print(f\"  Total chunks: {stats['count']}\")\n",
        "print(f\"  Average length: {stats['avg_length']:.0f} chars\")\n",
        "print(f\"  Min length: {stats['min_length']} chars\")\n",
        "print(f\"  Max length: {stats['max_length']} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Build FAISS Vector Store\n",
        "\n",
        "Generate embeddings and build the FAISS index.\n",
        "\n",
        "**Note**: This step requires an OpenAI API key and will incur costs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build and persist FAISS index\n",
        "# Using paragraph-based chunks as they provide good balance\n",
        "faiss_path = \"../faiss_index\"\n",
        "\n",
        "index_path = build_and_persist_faiss(\n",
        "    docs_paragraph,\n",
        "    faiss_path,\n",
        "    openai_api_key=os.environ.get('OPENAI_API_KEY')\n",
        ")\n",
        "\n",
        "print(f\"\u2713 FAISS index built and saved to: {index_path}\")\n",
        "print(f\"\\nNext: Open 02_rag_baseline.ipynb to use this index for retrieval\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we:\n",
        "- \u2713 Downloaded the GDPR PDF\n",
        "- \u2713 Parsed and split the document using multiple strategies\n",
        "- \u2713 Analyzed chunk statistics\n",
        "- \u2713 Built and persisted a FAISS vector store\n",
        "\n",
        "The FAISS index is now ready for use in the RAG pipeline."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}