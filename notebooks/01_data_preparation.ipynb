{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Preparation\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Downloading the GDPR PDF from official EU sources\n",
    "2. Parsing PDF with LangChain loaders\n",
    "3. Implementing chunking strategies (paragraph, article, token-based)\n",
    "4. Generating embeddings with OpenAI\n",
    "5. Building and persisting FAISS index\n",
    "\n",
    "**Note**: Set your `OPENAI_API_KEY` environment variable before running. Without it, the code runs in dry-run mode with placeholder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from src.data_prep import download_gdpr_pdf, load_and_split, build_and_persist_faiss, get_chunking_stats\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"OpenAI API Key configured: {'Yes' if os.getenv('OPENAI_API_KEY') else 'No (dry-run mode)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download GDPR PDF\n",
    "\n",
    "Download the official GDPR regulation PDF from EU sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GDPR PDF\n",
    "pdf_path = \"data/gdpr.pdf\"\n",
    "result_path = download_gdpr_pdf(pdf_path)\n",
    "\n",
    "print(f\"\\n✓ PDF saved to: {result_path}\")\n",
    "print(f\"File exists: {os.path.exists(result_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Chunk Documents\n",
    "\n",
    "Load the PDF and split it using different strategies:\n",
    "- **Paragraph**: Natural text breaks\n",
    "- **Article**: GDPR article boundaries\n",
    "- **Token**: Fixed-size chunks with overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different chunking strategies\n",
    "strategies = [\"paragraph\", \"article\", \"token\"]\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Strategy: {strategy.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    docs = load_and_split(pdf_path, strategy=strategy)\n",
    "    stats = get_chunking_stats(docs)\n",
    "    \n",
    "    print(f\"Number of chunks: {stats['num_chunks']}\")\n",
    "    print(f\"Average length: {stats['avg_length']:.0f} characters\")\n",
    "    print(f\"Min length: {stats['min_length']} characters\")\n",
    "    print(f\"Max length: {stats['max_length']} characters\")\n",
    "    \n",
    "    # Show sample chunk\n",
    "    if docs:\n",
    "        print(f\"\\nSample chunk:\\n{docs[0]['page_content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Select Optimal Strategy\n",
    "\n",
    "Based on the analysis, select the best chunking strategy for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use paragraph strategy (balanced approach)\n",
    "selected_strategy = \"paragraph\"\n",
    "documents = load_and_split(pdf_path, strategy=selected_strategy)\n",
    "\n",
    "print(f\"Selected strategy: {selected_strategy}\")\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"\\nReady for embedding generation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Embeddings and Build FAISS Index\n",
    "\n",
    "Create embeddings using OpenAI and build a FAISS index for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and persist FAISS index\n",
    "faiss_path = \"faiss_index/\"\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "index_path = build_and_persist_faiss(\n",
    "    docs=documents,\n",
    "    faiss_path=faiss_path,\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ FAISS index created and saved to: {index_path}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(\"1. Proceed to Notebook 2 for baseline RAG\")\n",
    "print(\"2. Use the FAISS index for retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "- ✓ Downloaded the GDPR PDF\n",
    "- ✓ Tested multiple chunking strategies\n",
    "- ✓ Generated document embeddings\n",
    "- ✓ Built and persisted FAISS index\n",
    "\n",
    "The FAISS index is now ready for retrieval in the RAG pipeline!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
