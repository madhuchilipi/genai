{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Milestone 1: Data Preparation\n",
        "\n",
        "This notebook demonstrates the data preparation pipeline for the GDPR RAG system:\n",
        "1. Download GDPR PDF document\n",
        "2. Parse and extract text using LangChain loaders\n",
        "3. Implement chunking strategies (paragraph, article, token-based)\n",
        "4. Generate embeddings using OpenAI\n",
        "5. Build FAISS vector store\n",
        "6. Persist index to disk\n",
        "\n",
        "## Setup\n",
        "\n",
        "Make sure you have set your `OPENAI_API_KEY` environment variable or in a `.env` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from src import data_prep\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"Data preparation module loaded successfully!\")\n",
        "print(f\"OpenAI API Key present: {bool(os.getenv('OPENAI_API_KEY'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Download GDPR PDF\n",
        "\n",
        "Download the official GDPR regulation PDF from the EU website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download GDPR PDF\n",
        "pdf_path = data_prep.download_gdpr_pdf(\"../data/gdpr.pdf\")\n",
        "print(f\"PDF path: {pdf_path}\")\n",
        "\n",
        "# Note: In dry-run mode (no network), this returns a placeholder path\n",
        "# TODO: Implement actual download for production use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and Parse PDF\n",
        "\n",
        "Use LangChain's document loaders to extract text from the PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and split the PDF\n",
        "# Try different chunking strategies\n",
        "\n",
        "# Strategy 1: Paragraph-based chunking\n",
        "chunks_paragraph = data_prep.load_and_split(\n",
        "    pdf_path, \n",
        "    strategy=\"paragraph\"\n",
        ")\n",
        "print(f\"\\nParagraph chunks: {len(chunks_paragraph)}\")\n",
        "\n",
        "# Strategy 2: Article-based chunking\n",
        "chunks_article = data_prep.load_and_split(\n",
        "    pdf_path,\n",
        "    strategy=\"article\"\n",
        ")\n",
        "print(f\"Article chunks: {len(chunks_article)}\")\n",
        "\n",
        "# Strategy 3: Token-based chunking\n",
        "chunks_token = data_prep.load_and_split(\n",
        "    pdf_path,\n",
        "    strategy=\"token\",\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "print(f\"Token chunks: {len(chunks_token)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Examine Chunks\n",
        "\n",
        "Let's examine the structure and content of our chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few chunks\n",
        "print(\"Sample chunks:\")\n",
        "for i, chunk in enumerate(chunks_paragraph[:3], 1):\n",
        "    print(f\"\\nChunk {i}:\")\n",
        "    print(f\"Content: {chunk['content'][:100]}...\")\n",
        "    print(f\"Metadata: {chunk['metadata']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Build FAISS Vector Store\n",
        "\n",
        "Generate embeddings and build the FAISS index for efficient retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build and persist FAISS index\n",
        "faiss_path = \"../faiss_index\"\n",
        "\n",
        "# Use the paragraph chunks for our index\n",
        "success = data_prep.build_and_persist_faiss(\n",
        "    chunks_paragraph,\n",
        "    faiss_path=faiss_path\n",
        ")\n",
        "\n",
        "if success:\n",
        "    print(f\"\\n✅ FAISS index built and saved to {faiss_path}\")\n",
        "else:\n",
        "    print(\"\\n❌ Failed to build FAISS index\")\n",
        "\n",
        "print(\"\\nNote: Full functionality requires OPENAI_API_KEY\")\n",
        "print(\"Without API key, the system runs in dry-run mode with placeholders\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Test Retrieval (Optional)\n",
        "\n",
        "If FAISS index was built successfully, test retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get example chunks for testing without API key\n",
        "example_chunks = data_prep.get_example_chunks()\n",
        "\n",
        "print(f\"Example chunks loaded: {len(example_chunks)}\")\n",
        "for i, chunk in enumerate(example_chunks, 1):\n",
        "    print(f\"\\n{i}. {chunk['content'][:80]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we:\n",
        "- ✅ Downloaded/prepared GDPR PDF data\n",
        "- ✅ Implemented multiple chunking strategies\n",
        "- ✅ Generated embeddings (with API key) or placeholders\n",
        "- ✅ Built and persisted FAISS vector store\n",
        "- ✅ Verified the data preparation pipeline\n",
        "\n",
        "Next: Proceed to `02_rag_baseline.ipynb` to build the baseline RAG system."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
